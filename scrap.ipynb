{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dependecies: \n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "003628e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the eBay filters dictionary \n",
    "\n",
    "url = \"https://www.ebay.com/sch/i.html\"\n",
    "# Defie the query parameters for the search request \n",
    "params = {\n",
    "    \"_nkw\": \"iphone 15 pro max\",\n",
    "    \"_sacat\": 0,\n",
    "    \"_from\": \"R40\",\n",
    "    \"LH_Sold\": 1,\n",
    "    \"LH_Complete\": 1,\n",
    "    \"rt\": \"nc\",\n",
    "    \"LH_ItemCondition\": \"2010|2020|2030\",\n",
    "    \"Network\": \"Unlocked\",\n",
    "    \"_dcat\": 9355,\n",
    "     \"_ipg\": 240,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d95ef171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define headers to mimic a real browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'DNT': '1',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30a7ff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ebay.com/sch/i.html?_nkw=iphone+15+pro+max&_sacat=0&_from=R40&LH_Sold=1&LH_Complete=1&rt=nc&LH_ItemCondition=2010%7C2020%7C2030&Network=Unlocked&_dcat=9355&_ipg=240\n"
     ]
    }
   ],
   "source": [
    "#Create a prepared reqeust to inspect the full URL with parameters\n",
    "\n",
    "request = requests.Request('GET', url, params=params)\n",
    "prepared_request = request.prepare()\n",
    "print(prepared_request.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4af6776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "No more items found - reached the end\n"
     ]
    }
   ],
   "source": [
    "#initlized Variable \n",
    "page_number = 0 \n",
    "items_list = []\n",
    "\n",
    "#loop over page \n",
    "while True: \n",
    "    page_number += 1\n",
    "    print(f\"Scraping page {page_number}...\")\n",
    "\n",
    "    params['_pgn'] = page_number\n",
    "\n",
    "    #Send get request to eBay with the defined parameters and headers: \n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    html_content = response.text # Get the HTML content of the page \n",
    "\n",
    "    #parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract items first\n",
    "    items = soup.find_all('div', class_='s-item__wrapper clearfix')\n",
    "    \n",
    "    # Check if no items found - means we've reached the end\n",
    "    if not items or len(items) <= 2:\n",
    "        print('No more items found - reached the end')\n",
    "        break\n",
    "    \n",
    "    # Extract Listings\n",
    "    for item in items[2:]:\n",
    "        title = item.find('div', class_='s-item__title').text\n",
    "        price = item.find('span', class_='s-item__price').text\n",
    "        link = item.find('a', class_='s-item__link')['href'].split('?')[0]\n",
    "        image_url = item.find('div', class_='s-item__image-wrapper image-treatment').find('img').get('src','No image URL')\n",
    "\n",
    "        # Define each item as a dictionary\n",
    "        item_dict = {\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Link': link,\n",
    "            'Image Link': image_url\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list\n",
    "        items_list.append(item_dict)\n",
    "    \n",
    "    # Find the next button to check if there are more pages\n",
    "    next_button = soup.find('button', class_='pagination__next', type='next')\n",
    "\n",
    "    # Check if the next button is disabled or doesn't exist\n",
    "    if not next_button or next_button.get('aria-disabled') == 'true':\n",
    "        print('No more pages to scrape')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bee265eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bfd3670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 items with class 's-item__wrapper clearfix'\n",
      "Found 0 items with class 's-item__wrapper'\n",
      "\n",
      "Found 0 divs with 's-item' in class name\n"
     ]
    }
   ],
   "source": [
    "# Debug: Let's check what we're actually getting from the page\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Check for items with the specific class\n",
    "items = soup.find_all('div', class_='s-item__wrapper clearfix')\n",
    "print(f\"Found {len(items)} items with class 's-item__wrapper clearfix'\")\n",
    "\n",
    "# Let's also try finding items with just 's-item__wrapper'\n",
    "items_alt = soup.find_all('div', class_='s-item__wrapper')\n",
    "print(f\"Found {len(items_alt)} items with class 's-item__wrapper'\")\n",
    "\n",
    "# Let's see what classes are actually available\n",
    "all_divs = soup.find_all('div', class_=lambda x: x and 's-item' in x)\n",
    "print(f\"\\nFound {len(all_divs)} divs with 's-item' in class name\")\n",
    "\n",
    "# Print first few class names to see the pattern\n",
    "if all_divs:\n",
    "    print(\"\\nFirst few div classes found:\")\n",
    "    for i, div in enumerate(all_divs[:5]):\n",
    "        print(f\"{i+1}. {div.get('class')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01faecee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 <li> elements with 's-item'\n"
     ]
    }
   ],
   "source": [
    "# Let's look for the actual item containers\n",
    "items_li = soup.find_all('li', class_=lambda x: x and 's-item' in str(x))\n",
    "print(f\"Found {len(items_li)} <li> elements with 's-item'\")\n",
    "\n",
    "if items_li:\n",
    "    print(\"\\nFirst item's classes:\")\n",
    "    print(items_li[0].get('class'))\n",
    "    \n",
    "    # Let's see the structure\n",
    "    print(\"\\nFirst item structure (first 500 chars):\")\n",
    "    print(str(items_li[0])[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ccf6f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total <li> elements: 0\n"
     ]
    }
   ],
   "source": [
    "# Let's try finding list items\n",
    "all_li = soup.find_all('li')\n",
    "print(f\"Total <li> elements: {len(all_li)}\")\n",
    "\n",
    "# Check for items in unordered list\n",
    "for li in all_li[:3]:\n",
    "    classes = li.get('class', [])\n",
    "    print(f\"\\nLI classes: {classes}\")\n",
    "    if classes:\n",
    "        print(f\"First 200 chars: {str(li)[:200]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe6b586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample HTML saved to ebay_sample.html\n",
      "Total HTML length: 13229 characters\n",
      "✗ Page does NOT contain 'iPhone'\n",
      "✗ Page does NOT contain price information\n"
     ]
    }
   ],
   "source": [
    "# Let's save a sample of the HTML to see what we're getting\n",
    "with open('ebay_sample.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(response.text[:50000])  # First 50k chars\n",
    "    \n",
    "print(\"Sample HTML saved to ebay_sample.html\")\n",
    "print(f\"Total HTML length: {len(response.text)} characters\")\n",
    "\n",
    "# Check if page contains certain keywords\n",
    "if 'iPhone' in response.text:\n",
    "    print(\"✓ Page contains 'iPhone'\")\n",
    "else:\n",
    "    print(\"✗ Page does NOT contain 'iPhone'\")\n",
    "    \n",
    "if 'Price' in response.text or 'price' in response.text:\n",
    "    print(\"✓ Page contains price information\")\n",
    "else:\n",
    "    print(\"✗ Page does NOT contain price information\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "886e7566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 script tags\n"
     ]
    }
   ],
   "source": [
    "# eBay likely embeds data in JSON within script tags\n",
    "# Let's find them\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Find all script tags\n",
    "scripts = soup.find_all('script', type=lambda x: x != 'text/javascript' or x is None)\n",
    "print(f\"Found {len(scripts)} script tags\")\n",
    "\n",
    "# Look for JSON data\n",
    "for i, script in enumerate(scripts[:20]):  # Check first 20 scripts\n",
    "    script_content = script.string\n",
    "    if script_content and ('itemId' in script_content or 'listingId' in script_content or 'price' in script_content.lower()):\n",
    "        print(f\"\\n=== Script {i} (first 500 chars) ===\")\n",
    "        print(script_content[:500])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "231a2f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Could not find srp-results list\n",
      "\n",
      "Trying to find any ul with 'results' in class...\n"
     ]
    }
   ],
   "source": [
    "# Let's try to find the srp-results ul\n",
    "results_list = soup.find('ul', class_='srp-results')\n",
    "if results_list:\n",
    "    print(\"✓ Found srp-results list!\")\n",
    "    items = results_list.find_all('li', recursive=False)\n",
    "    print(f\"  Found {len(items)} direct child <li> elements\")\n",
    "    if items:\n",
    "        print(\"\\nFirst item classes:\", items[0].get('class'))\n",
    "        print(\"\\nFirst item structure (first 800 chars):\")\n",
    "        print(str(items[0])[:800])\n",
    "else:\n",
    "    print(\"✗ Could not find srp-results list\")\n",
    "    # Try alternative\n",
    "    print(\"\\nTrying to find any ul with 'results' in class...\")\n",
    "    all_uls = soup.find_all('ul')\n",
    "    for ul in all_uls:\n",
    "        classes = ul.get('class', [])\n",
    "        if classes and any('result' in str(c).lower() for c in classes):\n",
    "            print(f\"Found ul with classes: {classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703625e",
   "metadata": {},
   "source": [
    "## Why items_list is empty (0 items)\n",
    "\n",
    "**Problem:** eBay is blocking the scraper with a bot detection challenge page.\n",
    "\n",
    "The HTML you're receiving says: \"Checking your browser before you access eBay\" - this is NOT the actual search results page.\n",
    "\n",
    "### Solutions:\n",
    "\n",
    "1. **Use Selenium with a real browser** - This can pass bot detection:\n",
    "   ```python\n",
    "   pip install selenium webdriver-manager\n",
    "   ```\n",
    "\n",
    "2. **Use eBay's Official API** - The proper way to get data:\n",
    "   - [eBay Finding API](https://developer.ebay.com/DevZone/finding/Concepts/MakingACall.html)\n",
    "   - [eBay Browse API](https://developer.ebay.com/api-docs/buy/browse/overview.html)\n",
    "   - Requires registration but is reliable and legal\n",
    "\n",
    "3. **Add delays and rotate User-Agents** - May help but not guaranteed:\n",
    "   - Add `time.sleep(random.uniform(2, 5))` between requests\n",
    "   - Rotate different user agents\n",
    "   - Use proxy servers\n",
    "\n",
    "4. **Use a scraping service** - Services like ScraperAPI, Bright Data handle anti-bot measures\n",
    "\n",
    "**Recommended:** Use eBay's official API for reliable, legal access to their data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a8366",
   "metadata": {},
   "source": [
    "## Selenium Implementation - Scraping with Real Browser\n",
    "\n",
    "This approach uses Selenium with Chrome to bypass bot detection by simulating a real user browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7bb215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d9cb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6de6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a57001c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af038db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075efbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
